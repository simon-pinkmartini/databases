{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: BBC 21st Century's greatest films\n",
    "\n",
    "Last year the BBC polled 177 film critics to get their picks for the best films of the century so far. While the BBC's aggregate pole is interesting, the long list include everyone who voted is perhaps more revealing from the data standpoint:\n",
    "\n",
    "http://www.bbc.com/culture/story/20160819-the-21st-centurys-100-greatest-films-who-voted\n",
    "\n",
    "Our goal for this project would be to scrape this page -- using beautiful soup and regular expressions -- in order to make a searchable database that would allow us to investigate all of the films listed by all of the critics.\n",
    "\n",
    "If you choose this project, the main challenge will be to come up with a database schema that organizes information by film as well as by critic. \n",
    "\n",
    "From a geocoding standpoint we would visualize this data set based on the country of the critics, and we could, with extra research, View the data set buy the country of the filmmaker.\n",
    "\n",
    "From an exploratory standpoint, here are some questions we could ask: \n",
    "\n",
    "1. Which countries have the most directors?\n",
    "2. Which directors have the most movies selected?\n",
    "3. What year had the most movies be selected?\n",
    "4. What are some other questions you can think of?\n",
    "\n",
    "The overall challenge, once the initial page is properly formatted into a database, is to find a way to enhance this data set with another source, and integrate that information with the information from the BBC poll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Supreme Court Arguments\n",
    "One group project I am proposing, would be to create a database of arguments before the US Supreme Court. The Supreme Court makes transcriptions of all arguments available on its website:\n",
    "\n",
    "https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx\n",
    "\n",
    "These arguments are available as PDFs. The downloading and conversion of PDFs to text is neither trivial nor particularly interesting of a challenge. The real challenge is what to do with the data once we have the text. \n",
    "\n",
    "Below, I show the code that I used to download all the PDFs of the the oral arguments before the Supreme Court from 2016, which I then convert to text files: that is where the fun starts. The goal of this project Will be to use regular expressions to parse the text to do oral arguments, that we can then search and measure the word spoken by the Supreme Court justices from case to case and across cases.\n",
    "\n",
    "The end goal is open: we want to have well structured data with the words of the justices entered. We also want to have a useful data set for each case that includes further information such as the final decision, the votes of each justice, who wrote the decision (and perhaps the dissenting opinion too). It is up to you what should ultimately be included and how we will be able to search through the data and view results.\n",
    "\n",
    "From an exploratory standpoint, here are some questions we could ask: \n",
    "\n",
    "1. Which justice speaks the most from case to case?\n",
    "2. What are the most frequent words used by each justice across cases?\n",
    "3. Does frequency of speech during arguments have any relationship toThe decision?\n",
    "4. What are some other questions you can think of?\n",
    "\n",
    "As far as geocoding goes: one way we could arrange this data on the map would be by finding the state where each of these cases originated. I have not found the best source for this, so it is an open question--part of this project will entail researching data sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I begin by scraping the links to all the transcriptions using **beautiful soup**--if you were to choose to do this project, give it also want to speak this page for the rest of its information such as the name of the case, the docket number, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_html = urlopen(\"https://www.supremecourt.gov/oral_arguments/argument_transcript.aspx\").read()\n",
    "soup_doc = BeautifulSoup(raw_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "the_table = soup_doc.find(class_=\"table datatables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_cases = the_table.find_all('td', attrs={'style': 'text-align:left'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2016_pdfs = []\n",
    "for the_link in the_cases:\n",
    "    all_2016_pdfs.append(the_link.a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2016_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I used the **requests** library (which I installed via pip) to download all of the PDFs to a folder on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for urls in all_2016_pdfs:\n",
    "    link = 'https://www.supremecourt.gov/oral_arguments/' + urls\n",
    "    book_name = \"/Users/Jon/Documents/columbia_syllabus/pdf/\" + link.split('/')[-1]\n",
    "    with open(book_name, 'wb') as book:\n",
    "        a = requests.get(link, stream=True)\n",
    "\n",
    "        for block in a.iter_content(512):\n",
    "            if not block:\n",
    "                break\n",
    "\n",
    "            book.write(block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here I make a list of the names of the PDFs\n",
    "pdf_names = [url.split('/')[-1] for url in all_2016_pdfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here I use the built in **os** library to run command line actions from Python. I am using the command line based **xpdf** tool (specifically its **pdftotext** command)that converts PDF to text in a way that's faster and simpler than using Python libraries that deal with PDFs. (This is certainly not the only way to do this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def pdf_to_text(name):\n",
    "    folder = \"/Users/Jon/Documents/columbia_syllabus/pdf/\"\n",
    "    input1 = folder + name\n",
    "    txt_name = name.replace(\".pdf\",\".txt\")\n",
    "    output1 = folder + txt_name\n",
    "    os.system(\"pdftotext '%s' '%s'\" % (input1, output1))\n",
    "\n",
    "#Here's an example of a single command    \n",
    "#os.system('pdftotext /Users/Jon/Documents/columbia_syllabus/pdf/16-605_2dp3.pdf /Users/Jon/Documents/columbia_syllabus/pdf/16-605_2dp3.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This those to the names and sends them to the function\n",
    "for pdf_file in pdf_names:\n",
    "    pdf_to_text(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/Users/Jon/Documents/columbia_syllabus/pdf/15-777_1b82.txt', 'r')\n",
    "sample_transcript = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
